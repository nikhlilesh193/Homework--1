# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rr79I95SMSvjM6Ghyo6nGrRFHbSs6t-v
"""

from collections import defaultdict, Counter


# -----------------------------
# Helper Functions
# -----------------------------

def add_end_of_word(words):
    """Add end-of-word marker '_' to each word."""
    return [" ".join(list(word)) + " _" for word in words]


def get_vocab(corpus):
    """Extract vocabulary from corpus."""
    vocab = set()
    for word in corpus:
        for token in word.split():
            vocab.add(token)
    return vocab


def get_bigram_counts(corpus):
    """Count bigram frequencies in the corpus."""
    counts = Counter()
    for word in corpus:
        symbols = word.split()
        for i in range(len(symbols) - 1):
            counts[(symbols[i], symbols[i + 1])] += 1
    return counts


def merge_bigram(corpus, pair):
    """Merge a bigram pair throughout the corpus."""
    merged = []
    bigram = " ".join(pair)
    replacement = "".join(pair)

    for word in corpus:
        merged.append(word.replace(bigram, replacement))
    return merged


# -----------------------------
# BPE Training
# -----------------------------

def learn_bpe(corpus, num_merges=10):
    """Learn BPE merges and print progress."""
    vocab = get_vocab(corpus)
    merges = []

    print("Initial vocabulary size:", len(vocab))
    print("-" * 40)

    for step in range(1, num_merges + 1):
        bigram_counts = get_bigram_counts(corpus)

        if not bigram_counts:
            break

        best_pair = bigram_counts.most_common(1)[0][0]
        merges.append(best_pair)

        corpus = merge_bigram(corpus, best_pair)
        vocab = get_vocab(corpus)

        print(f"Step {step}: merge {best_pair} → '{''.join(best_pair)}'")
        print("Vocabulary size:", len(vocab))
        print("-" * 40)

    return merges, corpus


# -----------------------------
# Word Segmentation
# -----------------------------

def segment_word(word, merges):
    """Segment a word using learned BPE merges."""
    tokens = list(word) + ["_"]
    tokens = " ".join(tokens)

    for pair in merges:
        bigram = " ".join(pair)
        replacement = "".join(pair)
        tokens = tokens.replace(bigram, replacement)

    return tokens.split()


# -----------------------------
# Main Execution
# -----------------------------

if __name__ == "__main__":

    # Toy corpus from the assignment
    raw_corpus = (
        ["low"] * 5
        + ["lowest"] * 2
        + ["newer"] * 6
        + ["wider"] * 3
        + ["new"] * 2
    )

    corpus = add_end_of_word(raw_corpus)

    # Learn BPE merges
    merges, final_corpus = learn_bpe(corpus, num_merges=10)

    # Words to segment
    test_words = ["new", "newer", "lowest", "widest", "newestest"]

    print("\nWord Segmentation Results")
    print("=" * 40)

    for word in test_words:
        segmented = segment_word(word, merges)
        print(f"{word:12s} → {' '.join(segmented)}")

from collections import Counter


# -----------------------------
# Preprocessing
# -----------------------------

def tokenize_text(text):
    """Simple whitespace tokenization."""
    return text.strip().split()


def add_eow(words):
    """Add end-of-word marker '_'."""
    return [" ".join(list(word)) + " _" for word in words]


# -----------------------------
# BPE Core Functions
# -----------------------------

def get_bigram_counts(corpus):
    counts = Counter()
    for word in corpus:
        symbols = word.split()
        for i in range(len(symbols) - 1):
            counts[(symbols[i], symbols[i + 1])] += 1
    return counts


def merge_pair(corpus, pair):
    bigram = " ".join(pair)
    replacement = "".join(pair)
    return [word.replace(bigram, replacement) for word in corpus]


def get_vocab(corpus):
    vocab = set()
    for word in corpus:
        vocab.update(word.split())
    return vocab


def learn_bpe(corpus, num_merges=30):
    merges = []
    merge_frequencies = []

    for _ in range(num_merges):
        counts = get_bigram_counts(corpus)
        if not counts:
            break

        best_pair, freq = counts.most_common(1)[0]
        merges.append(best_pair)
        merge_frequencies.append((best_pair, freq))
        corpus = merge_pair(corpus, best_pair)

    return merges, merge_frequencies, corpus


# -----------------------------
# Segmentation
# -----------------------------

def segment_word(word, merges):
    tokens = " ".join(list(word)) + " _"
    for pair in merges:
        tokens = tokens.replace(" ".join(pair), "".join(pair))
    return tokens.split()


# -----------------------------
# Main
# -----------------------------

if __name__ == "__main__":

    # Example paragraph (ENGLISH — replace with your own language if desired)
    paragraph = """
    Natural language processing enables computers to understand human language.
    Subword tokenization improves robustness for rare and unseen words.
    Byte Pair Encoding is widely used in modern NLP systems.
    """

    # Tokenize and prepare corpus
    words = tokenize_text(paragraph.lower())
    corpus = add_eow(words)

    # Train BPE
    merges, merge_freqs, final_corpus = learn_bpe(corpus, num_merges=30)

    vocab = get_vocab(final_corpus)

    # -----------------------------
    # Outputs required by assignment
    # -----------------------------

    print("\nTop 5 Most Frequent Merges")
    print("=" * 40)
    for pair, freq in merge_freqs[:5]:
        print(f"{pair} → '{''.join(pair)}' ({freq})")

    print("\nFive Longest Subword Tokens")
    print("=" * 40)
    for token in sorted(vocab, key=len, reverse=True)[:5]:
        print(token)

    # Words to segment
    test_words = [
        "language",      # common
        "tokenization",  # derived
        "robustness",    # medium
        "unseen",        # rare
        "nlpsystems"     # invented
    ]

    print("\nWord Segmentation")
    print("=" * 40)
    for word in test_words:
        print(f"{word:12s} → {' '.join(segment_word(word, merges))}")

paragraph_telugu = "మొన్న తెలుగు సినీ పరిశ్రమలో పెద్ద మార్పు వచ్చింది. కృష్ణా సినిమా షూటింగ్ ప్రారంభమైంది. మన రాష్ట్రంలో సాంకేతిక రంగంలో ప్రగతి కనిపిస్తోంది."

tokens_naive_telugu = paragraph_telugu.split()
print(tokens_naive_telugu)

!pip uninstall -y indicnlp
!pip install indic_nlp_library

from indicnlp.tokenize import indic_tokenize

tokens_tool = indic_tokenize.trivial_tokenize(paragraph_telugu)
print(tokens_tool)